# Default values for rosette-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  imageRegistry: ""
  imagePullSecrets: []

replicaCount: 1
indoccoref:
  enabled: false
image:
  registry:
  repository: rosette/server-enterprise
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
#  - name: secretName
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
# fsGroup: 2000

securityContext: {}
# capabilities:
#   drop:
#   - ALL
# readOnlyRootFilesystem: true
# runAsNonRoot: true
# runAsUser: 1000
initContainer:
  registry:
  image:
  tag:

service:
  type: ClusterIP
  port: 8181

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts: []
#  - host: chart-example.local
#    paths:
#    - path: /
#      pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  requests:
    ephemeral-storage: 2Gi
# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources, such as Minikube. If you do want to specify resources, uncomment the following
# lines and adjust them as necessary.
# limits:
#   cpu: 100m
#   memory: 128Mi
# requests:
#   cpu: 100m
#   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

probes:
  initialDelaySeconds: 60
  timeoutSeconds: 5
  periodSeconds: 30
  failureThreshold: 3

licenseSecretName: ""
storageClassName: ""
rootsVolumeName: ""
rootsAccessMode: "ReadWriteMany"
rootsResourceRequest: "150Gi"
rootsUseSelectorLabels: "true"
customProfilesVolumeClaimName: ""

apikeys:
  enabled: false

  persistentVolumeClaimName: ""
  # Adds write permissions for all users to the PVC's root directory's files
  volumePermissionOverride:
    enabled: false
    userId: 0

  dbName: "apikeys"
  dbAccessSecretName: ""
  backup:
    cronSchedule: "0 * * * *" #hourly
    # Restart failed backup job pods or create new ones for retries
    restartPolicy: Never
    # How long to keep a failed/completed backup job's pod
    ttlSecondsAfterFinished: 600
    # How many times a failed backup should be reattempted
    backoffLimit: 3

  # During upgrades a post hook job will wait for the main database to update before scaling down to one replica
  # This includes two wait processes, one for the pod to be running with the new revision's configuration and one for the database to be available
  # This value configures how many seconds to wait before failing the update for each process separately
  upgradeTimeoutSeconds: 300
  # Annotations for the database server scaling hook job
  hookAnnotations:
    "helm.sh/hook": post-install,post-upgrade,post-rollback
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
    "helm.sh/hook-weight": "0"

  service:
    type: ClusterIP
    port: 9092
  clusterDomain: cluster.local

  serviceAccount:
    create: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

  probes:
    initialDelaySeconds: 5
    timeoutSeconds: 10
    periodSeconds: 10
    failureThreshold: 5


# Some dynamically created persistent volumes will need their permissions updated before the
# roots can be extracted.  These parameters allow you to enable and control that process via an InitContainer.
volumePermissions:
  enabled: false
  initContainer:
    runAsUserId: 0
    imagePullPolicy: "IfNotPresent"
  rootVolume:
    chown:
      userId: 2001
      groupId: 0
    chmod:
      # Rosette Server needs at least read and execute permissions on the roots volume
      octalMode: 775
  securityContext:
    fsGroup: ""
    fsGroupChangePolicy: ""

# There are certain configuration settings that are set within an endpoint's data model directories, as opposed to the regular configuration directory.
# If you modify those settings and would like the changes to be carried through the helm upgrade/rollback process, you'll use this block to do so.
# For more details check the documentation and see the section in the README titled "Root configurations overrides."
rootsOverride:
  enabled: "false"
  overrideVolumeClaimName: ""
  backupVolumeClaimName: ""
#  separator: "&&&"
#  add:
#    - root: "rex"
#      targetPath: "/data/gazetteer/eng/accept/food.txt"
#      originPath: "pizza.txt"
#  override:
#    - root: "rex"
#      targetPath: "/data/gazetteer/eng/accept/food.txt"
#      originPath: "morefood.txt"
#  delete:
#    - root: "rex"
#      targetPath: "/data/gazetteer/eng/accept/food.txt"

rootsImageRepository: "rosette/"

# To change the annotations of the Rosette Roots extraction job, you can use the following block
# e.g. Argo CD cannot handle Helm hooks, so you can use this block to provide the jobs with appropriate annotations
# If not set it will provide the jobs with helm hook annotations for the charts default behaviour, which are the following:

#rootsExtraction:
#  upgrade:
#    annotations:
#      "helm.sh/hook": post-install,post-upgrade
#      "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
#      "helm.sh/hook-weight": "1"
#    podAnnotations: {}
#  rollback:
#    annotations:
#      "helm.sh/hook": post-rollback
#      "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
#      "helm.sh/hook-weight": "1"
#    podAnnotations: {}

### Uncomment for Argo CD; see Troubleshooting section for more information.
#rootExtraction:
#  upgrade:
#    annotations:
#      defaults: "false"

roots:
  version:
    ascent: 3.0.6.c79.0
    nlp4j: 2.0.6.c79.0
    rbl: 7.47.9.c79.0
    rct: 3.0.24.c79.0
    relax: 4.0.6.c79.0
    rex: 7.56.2.c79.0
    rli: 7.23.18.c79.0
    rnirnt: 7.51.0.c79.0
    tcat: 3.0.6.c79.0
    topics: 4.0.4.c79.0
    tvec: 7.0.7.c79.0

enabledEndpoints:
#- address-similarity
#- categories
#- entities
#- events
- language
#- morphology
#- name-deduplication
#- name-similarity
#- name-translation
#- record-similarity
#- relationships
#- semantics/similar
#- semantics/vector
#- sentences
#- sentiment
#- syntax/dependencies
#- tokens
#- topics
#- transliteration

enabledLanguages:
#- ara
#- deu
- eng
#- fas
#- fra
#- heb
#- hun
#- ind
#- ita
#- jpn
#- kor
#- nld
#- por
#- pus
#- qkp
#- rus
#- spa
#- swe
#- tgl
#- urd
#- vie
#- zho
#- zsm

config:
  com.basistech.downloadextract.cfg: |
    # downloaderExtractorConfigFilePathname is the pathname of downloader extractor configuration file.
    downloaderExtractorConfigFilePathname=${rosapi.config}/rosapi/downloaderExtractor.yaml
    # constraintsPathname is the pathname of constraints file.
    constraintsPathname=${rosapi.config}/rosapi/constraints.yaml
  com.basistech.rli.cfg: |
    # INTERNAL USE ONLY
    # This file compensates for the fact that the RLI factory config does not allow a default value for some options.
    # shortStringThreshold is the the minimum length of a long string, for short string processing.
    # The shortStringThreshold does not support as many languages as the long string algorithm.
    # values: change only if directed by engineering
    shortStringThreshold=141
  com.basistech.ws.addresses.cfg: |-
    # maxFieldLength is the maximum length of a field in an address to process
    # values: default is 500
    #maxFieldLength=500
  com.basistech.ws.apikeys.cfg: |
    # The connection mode to the apikeys database. Can be file or server. File is a database stored on disk and managed
    # by Rosette Server. Server is an H2 database server, which must be started separately.
    # Defaults to file
    #dbConnectionMode=file
    # Name of the database. Defaults to apikeys
    #dbName=apikeys
    # In file mode the path to the directory where the database is stored. Relative paths are based from the bin directory.
    # Defaults to ../db
    # In server mode this must be provided and should be <host>:[<port>][/<path>]. For more details see
    # https://www.h2database.com/html/features.html#database_url Server mode sections
    #dbURI=../db
    # When in server mode, use SSL to connect to the database. The database server must be running with SSL enabled.
    # Defaults to false.
    #dbSSLMode=false
    # Username for the database. For a new database this will be used to create the database. Defaults to rosette-server
    #dbUser=rosette-server
    # Password for the database. For a new database this will be used to create the database. Defaults to ""
    #dbPassword=
    # Do not validate the authorities of an API key. This means that every valid API key can access all licensed endpoints.
    # Can be true or false. Defaults to true
    #authenticationOnly=true
    # A comma separated list of antMatchers for endpoint paths that can be accessed without an API key.
    # As the url path base is configurable in com.basistech.ws.cxf there are two formats the paths can be defined
    # 1. Paths starting with '/' will be interpreted as is.
    # 2. Paths starting without '/' will be appended to the urlBase defined in com.basistech.ws.cxf
    # e.g. if the urlBase is the default '/rest': '/my-base/v1/info' -> '/my-base/v1/info'
    #                                             'v1/info' -> '/rest/v1/info
    # Read more about antMatchers at: https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/web/util/matcher/AntPathRequestMatcher.html
    # The default is doc/**,v1/info,v1/ping
    #unsecuredEndpoints=doc/**,v1/info,v1/ping
    # The number of days before an API key expires. Defaults to 90
    #expirationDays=90
  com.basistech.ws.cxf.cfg: |2

    # urlBase is the host and port for the apache cxf
    # values: must match the host/port entries in org.apache.cxf.http.jetty-main.cfg
    urlBase=http://0.0.0.0:${rosapi.port}/rest
  com.basistech.ws.dedupe.cfg: |
    # This file compensates for the fact that the RNI factory config does not allow a default value for this option.
    # maxNameLength sets the maximum name length for name deduplication
    # values: 500 is the recommended, and default, value.
    #maxNameLength=500
  com.basistech.ws.doc.cfg: |
    # docRoot is the path to the documentation
    docRoot=${rosapi.config}/../../doc
    # enable turns on the document server
    # values = true|false
    enable=true
  com.basistech.ws.fe.health.cfg: |
    # INTERNAL USE ONLY
    # outstandingRequestThreshold sets the maximum number of outstanding requests allowed before 'unhealthy' is returned.
    outstandingRequestThreshold=500
    # cloudwatchReport enables cloudwatch reporting of the total number of outstanding requests.
    #cloudwatchReport=true
    # cloudwatchReportInterval specifies the reporting interval.
    # This only applies if cloudwatchReport is true
    #cloudwatchReportInterval=PT1M (default)
    # asyncReponseTimeoutMs sets a timeout value for checking callback failure.
    # If for whatever reason the callback failed to show up, we would like to time it out
    # so that the queue size is decremented accordingly.
    # values: Make this slightly bigger than worker ELB timeout when deployed in AWS
    asyncResponseTimeoutMs=310000
    # Should the health/services endpoint list external services' health
    # Default is false
    #showExternalServicesHealth=false
  com.basistech.ws.frontend.cfg: |
    # INTERNAL USE ONLY
    # bindingVersions specifies the range of versions accepted from the client in [maven format](https://maven.apache.org/enforcer/enforcer-rules/versionRanges.html).
    bindingVersions=[0.10,2.0)
    # urlPrefix is appended to the CXF service prefix. We currently set this parameter to `/v1` and the CXF parameter to `/rest`, so that all of the front end URLs start with `/rest/v1`.
    urlPrefix=/v1
    # constraintsPathname is the pathname of the file defining request constraints
    constraintsPathname=${rosapi.config}/rosapi/constraints.yaml
    # profile data root folder that may contain profile-id/{rex,tcat} etc
    # Caveat:  The folder structure beneath the root will be different if rosapi.feature.CUSTOM_PROFILE_UNDER_APP_ID is active.
    #profile-data-root=file:///<where custom roots live>
  com.basistech.ws.local.usage.tracker.cfg: |
    # INTERNAL USE ONLY
    # suppress disables local usage tracker, default is false
    #suppress: true
    # report interval in minutes, default is 1 minute
    #reportInterval: 1
    # Usage tracker root customizes the location of the rosette-usage.yaml file
    # default location is  <rosette>/server/launcher/config.
    # Change it to preferred location. Example: /var/log
    #usage-tracker-root: /var/log
    # enabled is a new flag that can be used to enable/disable local usage tracking.
    # the suppress flag will be deprecated in a later release
    # default is true.
    # enabled: true
  com.basistech.ws.metrics.prometheus.cfg: ""
  com.basistech.ws.requesttracker.cfg: |
    # INTERNAL USE ONLY
    #
    # set mongoUri to a valid uri to enable request logging, it's recommend to use w=-1 for async logging
    # that doesn't block the response to the caller
    #   see http://docs.mongodb.org/manual/reference/connection-string/ for details
    #
    # mongoUri = mongodb://{host}:{port}/{raas_req_db}.{raas_req_collection}?w=-1&maxPoolSize=300
    # level    = none | partial | all
    #
    # For example
    # mongoUri is the address of the mongo instance
    #mongoUri=mongodb://localhost:27017/requestTrackerDb.requestTrackerCollection?w=-1&maxPoolSize=300
    # trackerLevel indicates what should be tracked
    # values: none | partial | all
    #trackerLevel=all
  com.basistech.ws.rni.cfg: |
    # This file compensates for the fact that the RNI factory config does not allow a default value for this option.
    # maxNameLength is the maximum length of a name to process
    # values: default is 500
    #maxNameLength=500
  com.basistech.ws.rnt.cfg: |
    # This file compensates for the fact that the RNI factory config does not allow a default value for this option.
    # maxNameLength is the maximum length of a name to process
    # values: default is 500
    #maxNameLength=500
  com.basistech.ws.tracker.logstash.cfg: |
    # logstashAddress is the host:port for the logstash tcp input plugin.
    #logstashAddress=
    # maxPendingMessages is he size of the blocking queue that stores the messages for transit. If this fills, new messages spill into the log.
    # Default is 10000
    #maxPendingMessages=
  com.basistech.ws.transport.embedded.cfg: |
    # DEPRECATED
    # Please configure workerThreadCount in com.basistech.ws.worker.cfg
  com.basistech.ws.transport.http.cfg: |
    # INTERNAL USE ONLY
    # rulesPathname is the pathname to transport rules
    rulesPathname=${rosapi.config}/rosapi/transport-rules.tsv
    # There's a desire to make this shorter to bail on things that have gotten stuck.
    # workerTimeout is the amount of time to allow a worker to complete a ticket before timing out and failing it. This is in ISO format, see the javadoc for `Duration#parse`. `PT30S` is '30 seconds.'
    workerTimeout=PT310S
    # Flow control parameters
    # workQueue.initialMaxOutstanding is for each distinct worker URL, the front-end will only allow this many requests to be pending.
    workQueue.initialMaxOutstanding=10
    # workQueue.decreaseThreshold controls the maxoutstanding value. If the average queue depth for a worker grows above this number, the 'maxOutstanding' value shrinks by decreaseAmount.
    workQueue.decreaseThreshold=20
    # workQueue.decreaseAmount controls the max number of outstanding requests by indicating how much to decrease the max number of outstanding requests when the queue size hits the 'decreaseThreshold'.
    workQueue.decreaseAmount=10
    # workQueue.increaseThreshold defines the max outstanding limit lower limit. If the queue size drops below this value, the max outstanding limit is increased by increaseAmount.
    workQueue.increaseThreshold=5
    # workQueue.increaseAmount controls how much to increase the max number of outstanding requests when the queue size falls to the increaseThreshold.
    workQueue.increaseAmount=10
    # workQueue.averageWindowTime controls the window time interval for averaging the queue size over this time interval, in ISO format.
    workQueue.averageWindowTime=PT1M
    # workQueue.adaptIntervalTime controls how often to compare the average queue size to the thresholds.
    workQueue.adaptIntervalTime=PT1M
    # maxHttpConnectionsPerUrl is the Maximum HTTP Connections Per URL default (1000).
    maxHttpConnectionsPerUrl=1000
    # Retry parameters
    # maxRetries is the number of maximum retries.
    maxRetries=3
    # maxRetryDelay is the maximum delay for retry. If it is longer than the delay, don't retry.
    maxRetryDelay=PT130S
    # retryBackoffMultiple is a factor to multiply with retryCount and retryBackoffTime to delay the retry.
    retryBackoffMultiple=2
    # retryBackoffTime is the etry backoff time.
    retryBackoffTime=PT0.05S
    # connectionTimeToLive controls how long HttpClient connections should be kept, default to 1-day given we have a custom resolver to monitor DNS changes now
    connectionTimeToLive=P1D
    # dataFormat is the transport serialization format. Values can be SMILE (default) or JSON.
    #dataFormat=SMILE
  com.basistech.ws.worker.cfg: |
    # configurationFilePathname is the pathname to the worker service configuration yaml.
    configurationFilePathname=${rosapi.config}/rosapi/worker-config.yaml
    # <component>-root is the pathname to the root directory for various component.
    rbl-root=${rosapi.roots}/rbl/7.47.9.c79.0
    rct-root=${rosapi.roots}/rct/3.0.24.c79.0
    ascent-root=${rosapi.roots}/ascent/3.0.6.c79.0
    tcat-root=${rosapi.roots}/tcat/3.0.6.c79.0
    rli-root=${rosapi.roots}/rli/7.23.18.c79.0
    rex-root=${rosapi.roots}/rex/7.56.2.c79.0
    flinx-root=${rosapi.roots}/rex/7.56.2.c79.0/flinx
    relax-root=${rosapi.roots}/relax/4.0.6.c79.0
    dp-root=${rosapi.roots}/nlp4j/2.0.6.c79.0
    rni-rnt-root=${rosapi.roots}/rni-rnt/7.51.0.c79.0
    tvec-root=${rosapi.roots}/tvec/7.0.7.c79.0
    topics-root=${rosapi.roots}/topics/4.0.4.c79.0
    # healthCheckQueueSizeThreshold is used for ELB health check, change the default as desired. Default is 20.
    #healthCheckQueueSizeThreshold=20
    # workerThreadTimeout is the timeout value for worker threads. To avoid diminished return or even degradation
    # It is recommended to keep the workerThreadTimeout < the maximum session (HTTP) timeout in RESTful.  In
    # embedded mode, the value is entirely up to the user.
    workerThreadTimeout=PT5M
    # workerThreadCount is the number of threads in the worker that are created to do the actual work. Default is 2.
    # it is probably best to not go above 2-3x the number of physical cores on the host machine.
    #workerThreadCount=2
    # cloudwatchReport indicates whether queue depth information will be reported by the worker to cloudwatch.
    #cloudwatchReport=false
    # cloudwatchReportInterval is the time interval in between reports to cloudwatch. The default is 'PT1M' -- report once per minute.
    #cloudwatchReportInterval=PT1M
    # warm up worker upon activation. The default is false.
    warmUpWorker=false
    # profile data root folder that may contain profile-id/{rex,tcat} etc
    # Caveat:  The folder structure beneath the root will be different if rosapi.feature.CUSTOM_PROFILE_UNDER_APP_ID is active.
    #profile-data-root=file:///<where custom roots live>
    # download and text extractor
    enableDte=true
    # profileDeletionMonitorInterval controls how often to check for a profile deletion. The default is PT5S.
    # Only used when profile-data-root is specified.
    profileDeletionMonitorInterval=PT5S
    # overrideEndpointsPathname declares an endpoint override yaml file allowing for selectively turning on/off specific endpoints. This may be used to aid in debugging processes.
    #overrideEndpointsPathname=${rosapi.config}/endpoint-override.yaml
  endpoint-override.yaml: |-
    # Use enabledEndpoints list instead
  jetty-ssl-config.xml: |-
    <beans xmlns="http://www.springframework.org/schema/beans"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xmlns:http="http://cxf.apache.org/transports/http/configuration"
           xmlns:httpj="http://cxf.apache.org/transports/http-jetty/configuration"
           xmlns:sec="http://cxf.apache.org/configuration/security"
           xsi:schemaLocation="
            http://www.springframework.org/schema/beans                 http://www.springframework.org/schema/beans/spring-beans.xsd
            http://cxf.apache.org/transports/http/configuration         http://cxf.apache.org/schemas/configuration/http-conf.xsd
            http://cxf.apache.org/transports/http-jetty/configuration   http://cxf.apache.org/schemas/configuration/http-jetty.xsd
            http://cxf.apache.org/configuration/security                http://cxf.apache.org/schemas/configuration/security.xsd">
        <httpj:engine-factory id="rosette-server-engine-config">
            <httpj:engine port="#{ systemProperties['rosapi.port'] }">
                <httpj:tlsServerParameters>
                    <sec:clientAuthentication required="false" />
                    <sec:keyManagers keyPassword="[key-pass]">
                        <sec:keyStore type="JKS" password="[keystore-pass]"
                                      file="path/to/keystore.jks"/>
                    </sec:keyManagers>
                    <sec:trustManagers>
                        <sec:keyStore type="JKS" password="[truststore-pass]"
                                      file="path/to/truststore.jks"/>
                    </sec:trustManagers>
                </httpj:tlsServerParameters>
            </httpj:engine>
        </httpj:engine-factory>
    </beans>
  org.apache.cxf.http.jetty-main.cfg: |
    # Reference: see HTTPJettyTransportActivator in CXF
    # The service is a managed service. So this file has to have "-xxx" at the end. This allows separate configurations for different ports; each such file gives parameters for a single port.
    # The host/port must match the urlBase property in com.basistech.ws.cxf.cfg.
    # host is the engine listen address.
    # values: hostname or IP
    host=0.0.0.0
    # port is the jetty port.
    port=${rosapi.port}
    # continuationsEnabled indicates that continuations will be checked.
    # values: true | false, default: true
    continuationsEnabled=true
    # reuseAddress allows the reuse of the service address.
    reuseAddress=true
    # maxIdleTime is the maximum idle time for a jetty connection. Timer is reset whenever there are any read or write actions on the underlying stream.
    maxIdleTime=330000
  org.apache.cxf.osgi.cfg: |
    # The contents of this file are not actually used since we are no longer using a servlet container. We assume that the only CXF services are us.
    # For the worker, this means urls like /rest/worker.
    # The client adds /v1 for /rest/v1 in com.basistech.frontend.cfg
    # org.apache.cxf.servlet.context is the prefix for all URLs for CXF services. We set this to `/rest`.
    org.apache.cxf.servlet.context=/rest


rosapi:
  analyze-factory-config.yaml: |
    #Whether to tokenize and analyze emoticons.
    emoticons: true
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    disambiguate: false
    #Whether to tokenize and analyze URLs.
    urls: true
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    analyze: true
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    universalPosTags: false
    #Whether to tokenize and analyze email addresses.
    emailAddresses: true
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Turns on fragment boundary detection.
    fragmentBoundaryDetection: true
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    #alternativeTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether to tokenize and analyze @mentions.
    #atMentions: false
    #Whether to tokenize and analyze hashtags.
    #hashtags: false
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  cat-factory-config.yaml: |
    #Sets the root directory. The file structure under this directory should look like:
    #models/
    #  eng/
    #    combined-iab-qag/...
    #
    #This option is required if the component configuration does not supply a 'modelDirectory'.
    rootDirectory: ${tcat-root}
    #Enable elbow thresholding
    #elbowThresholding: false
    #Maximum number of results to return
    #maxResults: Integer.MAX_VALUE
    #Uses 'elbow thresholding' to determine the number of results to return
    #
    #elbowThresholding: false
  constraints.yaml: |2

    # The following properties were determined as optimal during early rounds of performance tests targeting < 2 second response time.
    # Larger values may cause sustained degradation of system performance.  These values may change as components are improved.
    # maxInputRawByteSize is the maximum number of bytes per raw doc
    # recommend not to exceed 10,000,000
    maxInputRawByteSize   : 614400
    # maxInputRawTextSize is the maximum number of characters per submission
    # recommend not to exceed 1,000,000
    maxInputRawTextSize   : 50000
    # maxNameDedupeListSize is the maximum number of names to be deduplicated
    # recommend not to exceed  100,000.
    maxNameDedupeListSize : 1000
  default-only-tokenization-factory-config.yaml: |
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    analyze: false
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Turns on fragment boundary detection.
    #fragmentBoundaryDetection: true
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Whether to tokenize and analyze email addresses.
    #emailAddresses: false
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    #universalPosTags: false
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    #alternativeTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether to tokenize and analyze @mentions.
    #atMentions: false
    #Whether to tokenize and analyze hashtags.
    #hashtags: false
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to tokenize and analyze URLs.
    #urls: false
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    #disambiguate: true
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Whether to tokenize and analyze emoticons.
    #emoticons: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  downloaderExtractor.yaml: |
    # urlContentDownloader is the configuration for downloading URL content.
    urlContentDownloader:
      # processTimeoutMs is the timeout in milliseconds for processing URL downloader component.
      # values: dependent on network
      processTimeoutMs      : 10000
      # connectTimeout is the timeout in milliseconds for connecting to the URL.
      # values: dependent on network
      connectTimeout        : 5000
      # readTimeout is the timeout in milliseconds for reading content from URL.
      # values: dependent on network
      readTimeout           : 3000
      # maxRedirects is the maximum number of HTTP redirects allowed.
      # values: typically, lower is faster
      maxRedirects          : 10
      # maxRetries is the maximum number of retries to attempt.
      # values: dependent on network
      maxRetries            : 1
      # retryInterval is the retry interval in milliseconds.
      # values: arbitrary
      retryInterval         : 1000
    # proxy is the configuration for an OPTIONAL proxy.
    #proxy:
      # proxyType is the type of proxy (required).
      # values: HTTP or SOCKS
      #proxyType             : HTTP
      # inetSocketAddress is the IP of the proxy (required).
      #inetSocketAddress     : 10.2.3.4
      # inetSocketPort is the port of the proxy (required).
      #inetSocketPort        : 3128
      # username is the username for the proxy (optional).
      #username              : username
      # password is the password for the proxy (optional).
      #password              : password
    # textExtractor is the configuration for text extractor.
    textExtractor:
      # processTimeoutMs is the timeout in milliseconds for processing TextExtractionProcessor component.
      # values: arbitrary
      processTimeoutMs      : 10000
      # Deprecated
      # maxHeapSize is the maximum heap size of forked process in MB.
      # values: dependent on server
      # maxHeapSize           : 64
      # htmlExtractor is the HTML extractor class.
      htmlExtractor         : de.l3s.boilerpipe.extractors.ArticleExtractor
      # supportedParsers is the list of parsers to run (required).
      supportedParsers:
        - org.apache.tika.parser.html.HtmlParser
        - org.apache.tika.parser.txt.TXTParser
  dp-factory-config.yaml: |
    rootDirectory: ${dp-root}
    rootDirectory: ${dp-root}
    #language: { }
    #language: { }
  event-extractor-factory-config.yaml: |
    #Base URL of ETS in the format
    #http://<ETS_HOST>:<ETS_PORT>/ets (example: http://ets.customer.net:9999/ets) .
    #Set to empty to disable.
    eventTrainingServerUrl: null
    #Milliseconds to wait for the connection to be established.
    #0 for an infinite timeout.
    #-1 for system default.
    #connectionTimeout: -1
    #Proxy server port(optional).
    #proxyServerPort: not set
    #Milliseconds to wait for data to arrive.
    #0 for an infinite timeout.
    #-1 for system default.
    #socketTimeout: -1
    #Proxy server host(optional).
    #proxyServer: not set
  framework-override.properties: |
    # disable stacktrace in Felix SCR
    ds.showerrors=false
  rbl-factory-config.yaml: |
    #Whether to tokenize and analyze emoticons.
    emoticons: true
    #Whether to tokenize and analyze URLs.
    urls: true
    #Whether to tokenize and analyze hashtags.
    hashtags: true
    #Whether to tokenize and analyze @mentions.
    atMentions: true
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    alternativeTokenization: true
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    universalPosTags: true
    #Whether to tokenize and analyze email addresses.
    emailAddresses: true
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Turns on fragment boundary detection.
    #fragmentBoundaryDetection: true
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    #analyze: true
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    #disambiguate: true
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  rct-factory-config.yaml: |
    #The root directory of the RCT installation.
    #Providing this removes the need to provide a 'licenseString'
    rootDirectory: ${rct-root}
    #The XML string containing the RCT license.
    #
    #licenseString: ${rootDirectory}/licenses/rct-license.xml
    #When true, enables "reverse transliteration" (Arabic to Arabizi).
    # Otherwise, the transliterator will do normal transliteration (Arabizi to Arabic)
    #reversed: false
  relax-factory-config.yaml: |
    processors:
    - targeted
    - triggered
    - treePathPattern
    threadPoolSize: 1
    nlp4jRoot: ${dp-root}
    relaxRoot: ${relax-root}
    #confidenceModelPath: ""
    #entityFabricatorClassName: ""
    #adjunctsToArgs: false
    #generatePatternId: false
    #generateReport: false
    #splitConj: false
    #useRblTokensInternally: false
    #normalizePredicates: false
    #resolvePronouns: false
    #bundleLicenseString: ""
    #licenseString: ""
    #licenseFile: ""
    #allowedRelationships: [ ]
    #language: { }
    #processorTypes: [ ]
    #calculateConfidence: false
    #confidenceThreshold: 0.0
    #calculateSalience: false
    #removeNonSalient: false
  rex-factory-config.yaml: |
    # The REX root directory. A REX root directory contains language models and necessary configuration
    # files.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root" or "file:///path/to/rex-root"
    rootDirectory: ${rex-root}
    # The RBL root directory for REX to use.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root/rbl-je" or "file:///path/to/rex-root/rbl-je"
    rblRootDirectory: ${rex-root}/rbl-je
    #The entity types to be excluded from extraction.
    # This property accepts a list of values and the values are stored as Strings.
    # Valid values:  Person, Location, Organization, Product, Title
    # Other valid values are possible if you are using DB Pedia Types or have defined custom
    # entity types.
    #excludedEntityTypes: null
    # The option to search for and link mentions to knowledge base entities with disambiguation model.
    # Enabling this option also enables calculateConfidence.
    # Valid values:  true, false
    #linkEntities: false
    # Additional files used to produce statistical entities for the given language.
    # This property accepts a list of values and the values are comma separated trios.
    # The trios of values specify the language, case-sensitivity and the model file,
    # separated by commas. See caseSensitivity for valid values.
    # For example:
    # - eng,caseSensitive,english-model.bin
    # - jpn,automatic,japanese-model.bin
    #statisticalModels: null
    #Milliseconds to wait for data to arrive. 0 for an infinite timeout. -1 for system default.
    #socketTimeout: -1
    # An option to calculate entity-chain salience with statistical-based calculation
    # (returns 0 or 1) or simple calculation (returns score between 0 and 1).
    # Valid values: true, false
    #statSalienceMode: true
    # The confidence value threshold below which entities extracted by the statistical processor
    # are ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #confidenceThreshold: -1.0
    # The confidence value threshold below which linking results by the kbLinker processor are
    # ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #linkingConfidenceThreshold: -1.0
    # Register a custom processor class. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.class"
    #customProcessorClasses: null
    #Indoc-Coref server URL.
    #indocCorefServerUrl: not set
    # Apply joiner rules after redactor (old behavior).
    # Valid values: true, false
    #runJoinerPostRedactor: false
    # Additional gazetteer files used to produce entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/example.txt"
    #acceptGazetteers: null
    # Enables the splitting of IDENTIFIER:MONEY regex matches into IDENTIFIER:CURRENCY_AMT and IDENTIFIER:CURRENCY_TYPE
    # where the amount of the currency will be represented in one entity and the type will be
    # represented in another.
    #regexCurrencySplit: false
    # An option for document entity resolution (also known as entity chaining).
    # These values are enums and should not be quoted.
    # Valid values:  HIGH, STANDARD, STANDARD_MINUS or NULL
    #indocType: STANDARD
    # The option to disable usage of the disambiguation model when linking.
    # Valid values: true, false",
    #disableLinkerDisambiguation: false
    # Additional joiner rules files.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/to/custom-joiners/my-joiner-rules.xml"
    #joinerRuleFiles: null
    # The option to allow partial gazetteer matches. For the purposes of this setting, a partial
    # match is one that does not line up with token boundaries as determined by the internal
    # tokenizer. This only applies to accept gazetteers.
    # Valid values:  true, false
    #allowPartialGazetteerMatches: false
    # Extract entities in structured regions with NER (perceptron, dnn) models if there are enough
    # tokens.
    # Valid values:  Integers greater than or equal to 0.
    #structuredRegionProcessingSentenceTokensMin: 0
    # An option to adjust offsets according to normalization.
    # Valid values: true, false
    #adjustNormalizedOffsets: false
    # The capitalization (aka 'case') used in the input texts. Processing standard documents
    # requires caseSensitive, which is the default. Documents with all-caps, no-caps or headline
    # capitalization may yield higher accuracy if processed with the caseInsensitive value.
    # These values are enums and should not be quoted.
    # Valid values: automatic, caseSensitive, caseInsensitive
    #caseSensitivity: caseSensitive
    # The option to choose whether to look for candidate entities in text or link pre-existing mentions (extracted by other processors) to knowledge base entities with disambiguation model.
    # Valid values:  text, entities
    #linkMentionMode: text
    # An option to calculate entity confidence values.
    # Valid values:  true, false
    #calculateConfidence: false
    # The option to retain social media symbols ('@' and '#') in normalized output.
    # Valid values: true, false
    #retainSocialMediaSymbols: false
    # The overlay directory. An overlay directory is a directory shaped like the 'data' directory.
    # REX will look for files in both the overlay directory and the root directory, using files from
    # both locations. However, if a file exists in both places (as identified by its path relative
    # to the overlay or root data directory), REX prefers the version in the overlay directory.
    # If REX finds a zero-length file in the overlay directory, it ignores both that file and any
    # corresponding file in the root data directory.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/overlay-data" or "file:///path/to/overlay-data"
    #dataOverlayDirectory: null
    #Proxy server host for Indoc-Coref server (optional).
    #proxyServer: not set
    # The option to add supplemental regex files, usually for entity types that are excluded by
    # default. The supplemental regex files are located at data/regex/<lang>/accept/supplemental and
    # are not used unless specified.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/regex/<lang>/accept/supplemental/date-regexes.xml"
    # - "data/regex/<lang>/accept/supplemental/time-regexes.xml"
    #supplementalRegularExpressionPaths: null
    # Additional regex files used to reject entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/regexes.xml"
    #rejectRegularExpressionSets: null
    # The option to prefer length over weights during redaction. If true, the redactor will always
    # choose a longer entity over a shorter one if the two overlap, regardless of their user-defined
    # weights. In this case, if the lengths are the same, then weight is used to disambiguate the
    # entities. If false, the redactor will choose the higher weighted entity when two overlap,
    # regardless of the length of the entity string. In this case, if the weights are the
    # same, then the redactor will choose the longer of the two entities.
    # Valid values: true, false
    #redactorPreferLength: true
    # Regular expressions and gazetteers may be configured to match tokens partially independent
    # from token boundaries. If true, reported offsets correspond to token boundaries.
    # Valid values: true, false
    #snapToTokenBoundaries: true
    # The option to resolve pronouns to person entities.
    # Valid values: true, false
    #resolvePronouns: false
    # Enable the REX Training Server (RTS) decoder and specify the workspace to be used.
    # Valid values:  "aQuotedWorkspaceIdString" (Pre 2.0)
    #                "workspace/model" (2.0 and later)
    #rtsDecoder: null
    # Additional gazetteer files used to reject entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/example.txt"
    #rejectGazetteers: null
    # Custom list of Knowledge Bases for the linker, in order of priority.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/flinx/data/kb/<custom_kb_name>/"
    #kbs: null
    #Proxy server port for Indoc-Coref server (optional).
    #proxyServerPort: not set
    # The maximum number of entities for in-document coreference resolution (a.k.a. chaining).
    # Valid values:  Integers greater than 0.
    #maxResolvedEntities: 2000
    #Set to true to enable the indoc-coref-server. Valid values: true, false
    #useIndocServer: false
    # The maximum number of tokens allowed in an entity returned by Statistical Entity Extractor.
    # Entity Redactor discards entities from Statistical Entity Extractor with more than this number
    # of tokens.
    # Valid values:  Integers greater than 0.
    #maxEntityTokens: 8
    # List the set of active processors for an entity extraction run. All processors are active by
    # default. This method provides a way to turn off selected processors. The order of the
    # processors cannot be changed. Note that turning off redactor can cause overlapping and unsorted
    # entities to be returned.
    #
    # Default processors: acceptGazetteer, acceptRegex, rejectGazetteer, rejectRegex, statistical,
    #                     indocCoref, redactor, joiner
    #
    # Valid values: deepNeuralNetwork, indocCoref, acceptGazetteer, joiner, rejectRegex,
    #               statistical, nameClassifier, rejectGazetteer, acceptRegex, redactor, kbLinker,
    #               pronominalResolver
    #
    # These values are enums and should not be quoted.  Entries are added below in the form:
    # - acceptGazetteer
    # - acceptRegex
    #
    #processors: null
    #Milliseconds until a connection is established. 0 for an infinite timeout. -1 for system default.
    #connectionTimeout: -1
    # The option to extract social media entities.
    # Valid values: true, false
    #extractSocialMedia: false
    # Enable only the REX Training Server (RTS) decoder (rtsDecoder must be used with this
    # parameter)
    # Valid values:  true, false
    #onlyRtsDecoder: false
    # The option to keep existing annotated text entities.
    # Valid values: true, false
    #keepEntitiesInInput: false
    # The option to assign default confidence value 1.0 to non-statistical entities instead of
    # null.
    # Valid values: true, false
    #useDefaultConfidence: false
    # Set extraction method to use on structured regions.
    # These values are enums and should not be quoted.
    # Valid values:  nerModel, nameClassifier, none
    #structuredRegionProcessingType: NULL
    # Custom processors to add to annotators. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.java"
    #customProcessors: null
    # An option to calculate entity-chain salience values.
    # Valid values: true, false
    #calculateSalience: false
    # Additional files used to produce regex entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/regexes.xml"
    #acceptRegularExpressionSets: null
  rex-no-resolution-factory-config.yaml: |
    # The REX root directory. A REX root directory contains language models and necessary configuration
    # files.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root" or "file:///path/to/rex-root"
    rootDirectory: ${rex-root}
    # The RBL root directory for REX to use.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root/rbl-je" or "file:///path/to/rex-root/rbl-je"
    rblRootDirectory: ${rex-root}/rbl-je
    # The option to resolve pronouns to person entities.
    # Valid values: true, false
    resolvePronouns: false
    # An option to calculate entity confidence values.
    # Valid values:  true, false
    calculateConfidence: false
    # The capitalization (aka 'case') used in the input texts. Processing standard documents
    # requires caseSensitive, which is the default. Documents with all-caps, no-caps or headline
    # capitalization may yield higher accuracy if processed with the caseInsensitive value.
    # These values are enums and should not be quoted.
    # Valid values: automatic, caseSensitive, caseInsensitive
    caseSensitivity: automatic
    # The option to search for and link mentions to knowledge base entities with disambiguation model.
    # Enabling this option also enables calculateConfidence.
    # Valid values:  true, false
    linkEntities: true
    #The entity types to be excluded from extraction.
    # This property accepts a list of values and the values are stored as Strings.
    # Valid values:  Person, Location, Organization, Product, Title
    # Other valid values are possible if you are using DB Pedia Types or have defined custom
    # entity types.
    #excludedEntityTypes: null
    # Additional files used to produce statistical entities for the given language.
    # This property accepts a list of values and the values are comma separated trios.
    # The trios of values specify the language, case-sensitivity and the model file,
    # separated by commas. See caseSensitivity for valid values.
    # For example:
    # - eng,caseSensitive,english-model.bin
    # - jpn,automatic,japanese-model.bin
    #statisticalModels: null
    #Milliseconds to wait for data to arrive. 0 for an infinite timeout. -1 for system default.
    #socketTimeout: -1
    # An option to calculate entity-chain salience with statistical-based calculation
    # (returns 0 or 1) or simple calculation (returns score between 0 and 1).
    # Valid values: true, false
    #statSalienceMode: true
    # The confidence value threshold below which entities extracted by the statistical processor
    # are ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #confidenceThreshold: -1.0
    # The confidence value threshold below which linking results by the kbLinker processor are
    # ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #linkingConfidenceThreshold: -1.0
    # Register a custom processor class. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.class"
    #customProcessorClasses: null
    #Indoc-Coref server URL.
    #indocCorefServerUrl: not set
    # Apply joiner rules after redactor (old behavior).
    # Valid values: true, false
    #runJoinerPostRedactor: false
    # Additional gazetteer files used to produce entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/example.txt"
    #acceptGazetteers: null
    # Enables the splitting of IDENTIFIER:MONEY regex matches into IDENTIFIER:CURRENCY_AMT and IDENTIFIER:CURRENCY_TYPE
    # where the amount of the currency will be represented in one entity and the type will be
    # represented in another.
    #regexCurrencySplit: false
    # An option for document entity resolution (also known as entity chaining).
    # These values are enums and should not be quoted.
    # Valid values:  HIGH, STANDARD, STANDARD_MINUS or NULL
    #indocType: STANDARD
    # The option to disable usage of the disambiguation model when linking.
    # Valid values: true, false",
    #disableLinkerDisambiguation: false
    # Additional joiner rules files.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/to/custom-joiners/my-joiner-rules.xml"
    #joinerRuleFiles: null
    # The option to allow partial gazetteer matches. For the purposes of this setting, a partial
    # match is one that does not line up with token boundaries as determined by the internal
    # tokenizer. This only applies to accept gazetteers.
    # Valid values:  true, false
    #allowPartialGazetteerMatches: false
    # Extract entities in structured regions with NER (perceptron, dnn) models if there are enough
    # tokens.
    # Valid values:  Integers greater than or equal to 0.
    #structuredRegionProcessingSentenceTokensMin: 0
    # An option to adjust offsets according to normalization.
    # Valid values: true, false
    #adjustNormalizedOffsets: false
    # The option to choose whether to look for candidate entities in text or link pre-existing mentions (extracted by other processors) to knowledge base entities with disambiguation model.
    # Valid values:  text, entities
    #linkMentionMode: text
    # The option to retain social media symbols ('@' and '#') in normalized output.
    # Valid values: true, false
    #retainSocialMediaSymbols: false
    # The overlay directory. An overlay directory is a directory shaped like the 'data' directory.
    # REX will look for files in both the overlay directory and the root directory, using files from
    # both locations. However, if a file exists in both places (as identified by its path relative
    # to the overlay or root data directory), REX prefers the version in the overlay directory.
    # If REX finds a zero-length file in the overlay directory, it ignores both that file and any
    # corresponding file in the root data directory.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/overlay-data" or "file:///path/to/overlay-data"
    #dataOverlayDirectory: null
    #Proxy server host for Indoc-Coref server (optional).
    #proxyServer: not set
    # The option to add supplemental regex files, usually for entity types that are excluded by
    # default. The supplemental regex files are located at data/regex/<lang>/accept/supplemental and
    # are not used unless specified.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/regex/<lang>/accept/supplemental/date-regexes.xml"
    # - "data/regex/<lang>/accept/supplemental/time-regexes.xml"
    #supplementalRegularExpressionPaths: null
    # Additional regex files used to reject entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/regexes.xml"
    #rejectRegularExpressionSets: null
    # The option to prefer length over weights during redaction. If true, the redactor will always
    # choose a longer entity over a shorter one if the two overlap, regardless of their user-defined
    # weights. In this case, if the lengths are the same, then weight is used to disambiguate the
    # entities. If false, the redactor will choose the higher weighted entity when two overlap,
    # regardless of the length of the entity string. In this case, if the weights are the
    # same, then the redactor will choose the longer of the two entities.
    # Valid values: true, false
    #redactorPreferLength: true
    # Regular expressions and gazetteers may be configured to match tokens partially independent
    # from token boundaries. If true, reported offsets correspond to token boundaries.
    # Valid values: true, false
    #snapToTokenBoundaries: true
    # Enable the REX Training Server (RTS) decoder and specify the workspace to be used.
    # Valid values:  "aQuotedWorkspaceIdString" (Pre 2.0)
    #                "workspace/model" (2.0 and later)
    #rtsDecoder: null
    # Additional gazetteer files used to reject entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/example.txt"
    #rejectGazetteers: null
    # Custom list of Knowledge Bases for the linker, in order of priority.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/flinx/data/kb/<custom_kb_name>/"
    #kbs: null
    #Proxy server port for Indoc-Coref server (optional).
    #proxyServerPort: not set
    # The maximum number of entities for in-document coreference resolution (a.k.a. chaining).
    # Valid values:  Integers greater than 0.
    #maxResolvedEntities: 2000
    #Set to true to enable the indoc-coref-server. Valid values: true, false
    #useIndocServer: false
    # The maximum number of tokens allowed in an entity returned by Statistical Entity Extractor.
    # Entity Redactor discards entities from Statistical Entity Extractor with more than this number
    # of tokens.
    # Valid values:  Integers greater than 0.
    #maxEntityTokens: 8
    # List the set of active processors for an entity extraction run. All processors are active by
    # default. This method provides a way to turn off selected processors. The order of the
    # processors cannot be changed. Note that turning off redactor can cause overlapping and unsorted
    # entities to be returned.
    #
    # Default processors: acceptGazetteer, acceptRegex, rejectGazetteer, rejectRegex, statistical,
    #                     indocCoref, redactor, joiner
    #
    # Valid values: deepNeuralNetwork, indocCoref, acceptGazetteer, joiner, rejectRegex,
    #               statistical, nameClassifier, rejectGazetteer, acceptRegex, redactor, kbLinker,
    #               pronominalResolver
    #
    # These values are enums and should not be quoted.  Entries are added below in the form:
    # - acceptGazetteer
    # - acceptRegex
    #
    #processors: null
    #Milliseconds until a connection is established. 0 for an infinite timeout. -1 for system default.
    #connectionTimeout: -1
    # The option to extract social media entities.
    # Valid values: true, false
    #extractSocialMedia: false
    # Enable only the REX Training Server (RTS) decoder (rtsDecoder must be used with this
    # parameter)
    # Valid values:  true, false
    #onlyRtsDecoder: false
    # The option to keep existing annotated text entities.
    # Valid values: true, false
    #keepEntitiesInInput: false
    # The option to assign default confidence value 1.0 to non-statistical entities instead of
    # null.
    # Valid values: true, false
    #useDefaultConfidence: false
    # Set extraction method to use on structured regions.
    # These values are enums and should not be quoted.
    # Valid values:  nerModel, nameClassifier, none
    #structuredRegionProcessingType: NULL
    # Custom processors to add to annotators. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.java"
    #customProcessors: null
    # An option to calculate entity-chain salience values.
    # Valid values: true, false
    #calculateSalience: false
    # Additional files used to produce regex entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/regexes.xml"
    #acceptRegularExpressionSets: null
  rli-factory-config.yaml: |
    #Root directory of your RLI-JE installation.
    rootDirectory: ${rli-root}
    #This option is deprecated. Use 'languageWeightAdjustments'.
    #
    #A language hint.
    #
    #The weight is a float from 1 to 99 (the default is 1.0). Standard analysis and
    #short-string analysis use this hint differently.
    #
    #In standard analysis the hint reduces the distance of the input profile from the
    #specified language's profile by the specified weight (treated as a percentage).
    #For example "nld" with the default weight reduces the distance from the Dutch
    #language profile by 1.0%. Another example: "deu" with a weight of 50.005
    #reduces the distance of the input profile from the German language profile by
    #50.005%. If the language is already known, then use a large hint weight to
    #suppress language detection, leaving only encoding and script detection to be
    #performed.
    #
    #For short-string analysis, the hint weight provides a greater boost to
    #confidence than for standard analysis. Confidence is boosted by 100/(100 - weight).
    #For example, a 50% French weight boosts French confidence by a factor of 2, and
    #75% French weight boosts confidence by a factor of 4.
    #languageHint: null
    #The short-string threshold.
    #
    #By default, the short-string threshold is 0 and short-string language detection
    #is inactive. To turn it on, set the threshold to a non-negative integer, such as
    #6. If the string contains fewer characters than this threshold, RLI-JE performs
    #short-string language detection. When RLI-JE is performing short-string language
    #detection, only 'languageHint' and language weight adjustments are used. The
    #other options are ignored.
    #shortStringThreshold: 0
    #Whether to force a language boundary when the script changes
    #breakRegionOnScriptBoundary: true
    #The maximum number of results to return.
    #maxResults: 5
    #The invalidity threshold.
    #
    #If the input profile distance measure is smaller than invalidityThreshold/100 ×
    #maximumProfileDistance, the result is flagged as valid. maximumProfileDistance
    #is the maximum distance any input text can have. If the threshold is 0, all
    #results are flagged as invalid. If the value is 100, all results are flagged
    #valid. The accepted value range is [0, 100].
    #invalidityThreshold: 99.0
    #Enables language region detection.
    #multilingual: false
    #The minimum length for a language region (the amount of text examined in a
    #script region). This is only used for language region detection.
    #minRegionLength: 20
    #The minimum number of valid non-whitespace characters required for
    #identification. If the input data has less than the minimum number of valid
    #characters, no detection is performed.
    #minValidChars: 4
    #The maximum region length. This is only used for language region detection.
    #maxRegionLength: 65536
    #A set of language weight adjustments.
    #
    #Use this setting to change the language weight for a language, for the purpose
    #of detecting another language that is present in the document. The weight
    #parameter is a non-negative percentage that is applied to the language weight;
    #70 reduces the weight to 70% of its default value. For example, you may be
    #interested in detecting German in a document that is mostly English but also
    #contains some German. Reduce the language weight for English or increase the
    #language weight for German.
    #languageWeightAdjustments: [ ]
    #This option is deprecated. Use 'languageWeightAdjustments'.
    #
    #An encoding hint.
    #
    #The weight is a float from 1 to 100 (the default is 3.0). The hint reduces the
    #distance from the input profile from the specified encoding's profile by the
    #specified weight (treated as a percentage). For example, "Ascii" with the
    #default weight reduces the distance from the input profile by 1.0%. The value of
    #100 forces only those results which match the encoding hint to be considered
    #during detection.
    #encodingHint: null
    #Whether to ignore script and encoding differences in language detection results.
    #By default, results that share a language but have different scripts or
    #encodings, such as Simplified Chinese and Traditional Chinese, are returned as
    #separate results. If script and encoding differences are ignored, only one
    #result is returned per language.
    #uniqueLanguages: false
    #The maximum number of results to return.
    #minNonScriptioContinuaRegionLength: 5
    #Enables language region detection.
    #koreanDialects: false
    #The XML license content. This overrides 'licensePath'.
    #licenseString: ${rootDirectory}/licenses/rlp-license.xml
    #The profile depth.
    #
    #Profile depth is the maximum number of n-grams to be used in the input profile.
    #If the depth is 100, the 100 most frequent n-grams are included in the input
    #profile. A small depth improves detection speed but reduces detection accuracy.
    #profileDepth: 1200
    #Path to a file containing your RLI-JE XML license. If you don't provide this
    #option, or 'licenseString', RLI-JE assumes your license is
    #"${rootDirectory}/licenses/rlp-license.xml".
    #licenseFile: ${rootDirectory}/licenses/rlp-license.xml
    #The ambiguity threshold.
    #
    #If the distance difference between the input profile and a candidate built-in
    #profile is smaller than ambiguityThreshold/100 × bestProfileDistance, the result
    #is flagged as ambiguous. bestProfileDistance is the distance measure of the best
    #matching profile. If the threshold is 0 then all results will be flagged as
    #unambiguous. If the value is 100 then all results will be flagged as ambiguous.
    #The accepted value range is [0, 100].
    #ambiguityThreshold: 2.0
  rni-dedupe-factory-config.yaml: |
    #The path to the root directory.
    rootDirectory: ${rni-rnt-root}
    #The XML license content.
    #licenseString: null
  rni-factory-config.yaml: ""
  rnt-factory-config.yaml: |
    #The path to the root directory
    rootDirectory: ${rni-rnt-root}
    #The XML license content.
    #licenseString: null
    #A mapping of languages to transliteration schemes.
    #schemeMap: null
  semantic-vectors-factory-config.yaml: |
    rblRootDirectory: ${rbl-root}
    rootDirectory: ${tvec-root}
    #numClosest: 0
    #queryLanguage: { }
    #resultLanguages: [ ]
    #embeddingsMode: { }
    #forceReadOOV: { }
    #allowWriteOOV: { }
    #language: { }
    #granularities: [ ]
    #embeddingsMode: { }
    #maxResults: 0
  sent-factory-config.yaml: |
    #Sets the root directory.
    #The file structure under this directory should look like:
    #data/
    #  dnn/
    #    eng/...
    #  svm/
    #    eng/...
    #    spa/...
    #licenses/
    #  rlp-license.xml
    #This option is required.
    rootDirectory: ${ascent-root}
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #Set the type of model.
    #Options are SVM and DNN.
    #modelType: SVM
    #The path of the license file.
    #If not set, a default model will be used.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
  similar-terms-factory-config.yaml: |
    rblRootDirectory: ${rbl-root}
    rootDirectory: ${tvec-root}
    #numClosest: 0
    #queryLanguage: { }
    #resultLanguages: [ ]
    #embeddingsMode: { }
    #forceReadOOV: { }
    #allowWriteOOV: { }
    #language: { }
    #granularities: [ ]
    #embeddingsMode: { }
    #maxResults: 0
  tokenization-factory-config.yaml: |
    #Whether to tokenize and analyze emoticons.
    emoticons: true
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    disambiguate: false
    #Whether to tokenize and analyze URLs.
    urls: true
    #Whether to tokenize and analyze hashtags.
    hashtags: true
    #Whether to tokenize and analyze @mentions.
    atMentions: true
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    analyze: false
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    alternativeTokenization: true
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    universalPosTags: false
    #Whether to tokenize and analyze email addresses.
    emailAddresses: true
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Turns on fragment boundary detection.
    #fragmentBoundaryDetection: true
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  topics-factory-config.yaml: |
    #the path to the topics data root directory
    rootDirectory: ${topics-root}
  transport-rules.tsv: "/address-similarity\t*\tlocal:\n/categories\t*\tlocal:\n/dte\t\
    *\tlocal:\n/entities\t*\tlocal:\n/events\t*\tlocal:\n/language\t*\tlocal:\n/morphology\t\
    *\tlocal:\n/name-deduplication\t*\tlocal:\n/name-similarity\t*\tlocal:\n/name-translation\t\
    *\tlocal:\n/relationships\t*\tlocal:\n/semantics/similar\t*\tlocal:\n/semantics/vector\t\
    *\tlocal:\n/sentences\t*\tlocal:\n/sentiment\t*\tlocal:\n/syntax/dependencies\t\
    *\tlocal:\n/text-embedding\t*\tlocal:\n/tokens\t*\tlocal:\n/topics\t*\tlocal:\n\
    /transliteration\t*\tlocal:\n"
  worker-config.yaml: |
    # components: defines a list of component
    # Each component sets up one or more SDK factories, as defined in the
    # rosette-osgi project. These are objects that implement
    # `RosetteComponentService`, and can deliver SDK factories in return for
    # YAML configuration of those factories.
    components:
    # categorization via tcat
    - componentName: categorization
      factories:
        default: cat-factory-config.yaml
    - componentName: dependency-parsing
      factories:
        default: dp-factory-config.yaml
    # RBL
    - componentName: base-linguistics
      factories:
        default: rbl-factory-config.yaml
        statistically-tokenize: default-only-tokenization-factory-config.yaml
        tokenize: tokenization-factory-config.yaml
        analyze: analyze-factory-config.yaml
    # RCT
    - componentName: arabic-chat-transliterator
      factories:
        default: rct-factory-config.yaml
    # REX
    - componentName: entity-extraction
      factories:
        default: rex-factory-config.yaml
        no-resolution: rex-no-resolution-factory-config.yaml
    # Events
    - componentName: event-extractor
      factories:
        default: event-extractor-factory-config.yaml
    # RELAX
    - componentName: relationship-extraction
      factories:
        default: relax-factory-config.yaml
    # RLI
    - componentName: language-identification
      factories:
        default: rli-factory-config.yaml
    # RNT
    - componentName: name-translation
      factories:
        default: rnt-factory-config.yaml
    # RNI
    - componentName: name-similarity
      factories:
        default: rni-factory-config.yaml
    # RNI Dedupe
    - componentName: name-deduplication
      factories:
        default: rni-dedupe-factory-config.yaml
     #sentiment via tcat
    - componentName: sentiment
      factories:
        default: sent-factory-config.yaml
    # TVEC
    - componentName: semantic-vectors
      factories:
        default: semantic-vectors-factory-config.yaml
    - componentName: similar-terms
      factories:
        default: similar-terms-factory-config.yaml
    # Topics
    - componentName: topics
      factories:
        default: topics-factory-config.yaml
    # Note: We don't have a factory because there's no SDK service
    - componentName: address-similarity
    - componentName: record-similarity
    #
    # Pipelines
    #
    # To put the factories to work, the configuration defines a series
    # of pipelines. Each pipeline handles a endpoint+language pair; either
    # language or endpoint can be '*' to mean 'any'.  The pipeline is a
    # series of components to run, defined in `steps`. For each component,
    # the configuration calls out the factory for it's SDK.
    # To specify any language, write `[ '*' ]`. Genre is optional.
    textPipelines:
    - endpoint: /syntax/dependencies
      languages: [ '*' ]
      steps:
      - componentName: dependency-parsing
    # Language detection. Not used, done in front end, delete?
    - endpoint: /language
      languages: [ '*' ]
      steps:
      - componentName: language-identification
    # Morphology
    - endpoint: /morphology
      languages: [ '*' ]
      steps:
      - componentName: base-linguistics
    # Entities
    - endpoint: /entities
      languages: [ '*' ]
      steps:
      - componentName: entity-extraction
      # events
    - endpoint: /events
      languages: [ '*' ]
      steps:
        - componentName: event-extractor
    # relationships
    - endpoint: /relationships
      languages: [ '*' ]
      steps:
      - componentName: base-linguistics
        factoryName: analyze
      - componentName: entity-extraction
      - componentName: dependency-parsing
      - componentName: relationship-extraction
    # sentiment
    - endpoint: /sentiment
      languages: [ 'ara', 'eng', 'fas', 'fra', 'jpn', 'spa', 'uen' ]
      steps:
      - componentName: entity-extraction
      - componentName: sentiment
    # categories
    - endpoint: /categories
      languages: [ 'eng', 'uen' ]
      steps:
      - componentName: base-linguistics
        factoryName: tokenize
      - componentName: categorization
    # semantic vectors
    - endpoint: /semantics/vector
      languages: [ 'ara', 'deu', 'eng', 'fas', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor', 'por', 'qkp', 'qkr', 'rus', 'spa', 'tgl', 'urd', 'zho' ]
      steps:
      - componentName: base-linguistics
        factoryName: statistically-tokenize
      - componentName: semantic-vectors
    - endpoint: /text-embedding
      languages: [ 'ara', 'deu', 'eng', 'fas', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor', 'por', 'qkp', 'qkr', 'rus', 'spa', 'tgl', 'urd', 'zho' ]
      steps:
      - componentName: base-linguistics
        factoryName: statistically-tokenize
      - componentName: semantic-vectors
    # similar terms
    - endpoint: /semantics/similar
      languages: [ 'ara', 'deu', 'eng', 'fas', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor', 'por', 'qkp', 'qkr', 'rus', 'spa', 'tgl', 'urd', 'zho' ]
      steps:
      - componentName: base-linguistics
        factoryName: statistically-tokenize
      - componentName: similar-terms
    # sentences
    - endpoint: /sentences
      languages: [ '*' ]
      steps:
        - componentName: base-linguistics
          factoryName: tokenize
    # tokens
    - endpoint: /tokens
      languages: [ '*' ]
      steps:
        - componentName: base-linguistics
          factoryName: tokenize
    # transliteration
    - endpoint: /transliteration
      languages: [ '*' ]
      steps:
      - componentName: arabic-chat-transliterator
    # topics
    - endpoint: /topics
      languages: [ 'eng' ]
      steps:
      - componentName: base-linguistics
        factoryName: analyze
      - componentName: entity-extraction
        factoryName: no-resolution
      - componentName: topics
    # Non-TextPipelines
    otherEndpoints:
    - endpoint: /name-translation
      componentName: name-translation
    - endpoint: /name-similarity
      componentName: name-similarity
    - endpoint: /name-deduplication
      componentName: name-deduplication
    - endpoint: /address-similarity
      componentName: address-similarity
    - endpoint: /record-similarity
      componentName: record-similarity


conf:
  apikey-console.log4j2.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <Configuration monitorInterval="60">
        <Properties>
            <Property name="log-path">../apikey-logs</Property>
            <Property name="archive">${log-path}/archive</Property>
        </Properties>
        <Appenders>
            <Console name="Console-Appender" target="SYSTEM_OUT">
                <PatternLayout>
                    <pattern>
                        [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n
                    </pattern>>
                </PatternLayout>
            </Console>
        </Appenders>
        <Loggers>
            <Root level="warn">
                <AppenderRef ref="Console-Appender" level="warn"/>
            </Root>
            <!-- Turn off Hibernate SQL exception logging -->
            <Logger name="org.hibernate.engine.jdbc.spi.SqlExceptionHelper" level="off"/>
        </Loggers>
    </Configuration>
  java_opts.conf: ""
  log4j2.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <Configuration monitorInterval="60">
        <Properties>
            <Property name="log-path">../logs</Property>
            <Property name="archive">${log-path}/archive</Property>
        </Properties>
        <Appenders>
            <Console name="Console-Appender" target="SYSTEM_OUT">
                <PatternLayout>
                    <pattern>
                        [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n
                    </pattern>>
                </PatternLayout>
            </Console>
            <RollingFile name="File-Appender"
                         fileName="${log-path}/rosapi.log"
                         filePattern="${archive}/rosapi.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
    <!--
            <RollingFile name="Request-Appender"
                         fileName="${log-path}/request-tracker.log"
                         filePattern="${archive}/request-tracker.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
    -->
            <RollingFile name="CXF-Request-Appender"
                         fileName="${log-path}/cxf-request-logger.log"
                         filePattern="${archive}/cxf-request-logger.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
            <RollingFile name="500-Exception-Appender"
                         fileName="${log-path}/500-exception.log"
                         filePattern="${archive}/500-exception.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <RegexFilter regex=".*Exception processing ticket.*" onMatch="ACCEPT" onMismatch="DENY"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
        </Appenders>
        <Loggers>
            <Root level="warn">
                <AppenderRef ref="File-Appender" level="warn"/>
                <AppenderRef ref="Console-Appender" level="warn"/>
            </Root>
            <Logger name="org.eclipse.jetty.server.handler" level="error"/>
    <!--
            <Logger name="com.basistech.ws.logrequesttracker" level="info" additivity="false">
                <AppenderRef ref="Request-Appender"/>
            </Logger>
    -->
    <!--
            <Logger name="org.apache.cxf.services" level="info">
                <AppenderRef ref="CXF-Request-Appender"/>
                <AppenderRef ref="Console-Appender"/>
            </Logger>
    -->
            <Logger name="com.basistech.ws" level="info">
                <AppenderRef ref="500-Exception-Appender" level="info"/>
            </Logger>
        </Loggers>
    </Configuration>
  logging.properties: |
    org.apache.aries.spifly = ERROR
    org.apache.cxf.bus.blueprint = ERROR
  wrapper-license.conf: |-
    #encoding=UTF-8
    wrapper.license.type=DEV
    wrapper.license.id=202411270000001
    wrapper.license.licensee=Basis Technology Corp.
    wrapper.license.group=Rosette API
    wrapper.license.dev_application=Analytics by Babel Street
    wrapper.license.features=pro, 64bit
    wrapper.license.upgrade_term.begin_date=2010-11-09
    wrapper.license.upgrade_term.end_date=2025-11-09
    wrapper.license.key.1=6e79-b642-320d-4283
    wrapper.license.key.2=d701-8070-5923-82fd
    wrapper.license.key.3=1f59-be50-c42a-ef0b
    wrapper.license.key.4=db90-75f2-8b12-51d6
  wrapper.conf: |
    #encoding=UTF-8
    # Configuration files must begin with a line specifying the encoding
    #  of the the file.
    #********************************************************************
    # Wrapper License Properties (Ignored by Community Edition)
    #********************************************************************
    # Professional and Standard Editions of the Wrapper require a valid
    #  License Key to start.  Licenses can be purchased or a trial license
    #  requested on the following pages:
    # http://wrapper.tanukisoftware.com/purchase
    # http://wrapper.tanukisoftware.com/trial
    # Include file problems can be debugged by leaving only one '#'
    #  at the beginning of the following line:
    ##include.debug
    # The Wrapper will look for either of the following optional files for a
    #  valid License Key.  License Key properties can optionally be included
    #  directly in this configuration file.
    #include ../conf/wrapper-license.conf
    #include ../conf/wrapper-license-%WRAPPER_HOST_NAME%.conf
    # The following property will output information about which License Key(s)
    #  are being found, and can aid in resolving any licensing problems.
    #wrapper.license.debug=TRUE
    #********************************************************************
    # Wrapper Localization
    #********************************************************************
    # Specify the language and locale which the Wrapper should use.
    wrapper.lang=en_US # en_US or ja_JP
    # Specify the location of the language resource files (*.mo).
    wrapper.lang.folder=../lang
    #********************************************************************
    # Wrapper Java Properties
    #********************************************************************
    # Java Application
    #  Locate the java binary on the system PATH:
    wrapper.java.command=%JAVA_HOME%/bin/java
    #  Specify a specific java binary:
    #set.JAVA_HOME=/java/path
    #wrapper.java.command=%JAVA_HOME%/bin/java
    # Tell the Wrapper to log the full generated Java command line.
    wrapper.java.command.loglevel=INFO
    #********************************************************************
    # Basis Technology Customizations
    #********************************************************************
    # Java Main class.  This class must implement the WrapperListener interface
    #  or guarantee that the WrapperManager class is initialized.  Helper
    #  classes are provided to do this for you.
    #  See the following page for details:
    #  http://wrapper.tanukisoftware.com/doc/english/integrate.html
    wrapper.java.mainclass=com.basistech.ws.launcher.tanuki.RosapiTanukiLauncher
    # Log level for notices about missing Java Classpath entries.
    wrapper.java.classpath.missing.loglevel=WARN
    # Java Classpath (include wrapper.jar)  Add class path elements as
    #  needed starting from 1
    wrapper.java.classpath.1=../framework/*.jar
    wrapper.java.classpath.2=../lib/wrapper.jar
    # Java Library Path (location of Wrapper.DLL or libwrapper.so)
    wrapper.java.library.path.1=../lib
    # Java Bits.  On applicable platforms, tells the JVM to run in 32 or 64-bit mode.
    #wrapper.java.additional.auto_bits=TRUE
    #********************************************************************
    # Basis Technology Customizations
    #********************************************************************
    # Rosette Enterprise will ignore 'rosapi roots' by spotting the existence of the 'roots' directory.
    # Java Additional Parameters
    wrapper.java.additional.101=-Drosapi.roots=%ROSAPI_ROOTS%
    wrapper.java.additional.102=-Dcom.sun.management.jmxremote
    wrapper.java.additional.103=-Drosapi.config=../launcher/config
    wrapper.java.additional.104=-Drosapi.port=8181
    # Cloud-specific (2xx) customizations are added to the bottom this file.
    # WS-2248
    wrapper.java.additional.302=-Dorg.bytedeco.javacpp.maxPhysicalBytes=0
    # WS-2622: disable crypto-policies alignment
    # https://access.redhat.com/documentation/en-us/openjdk/17/html/configuring_openjdk_17_on_rhel_with_fips/config-fips-in-openjdk
    wrapper.java.additional.331=-Djava.security.disableSystemPropertiesFile=true
    # Feature flags (4xx)
    #wrapper.java.additional.401=-Drosapi.feature.CUSTOM_PROFILE_UNDER_APP_ID=true
    #wrapper.java.additional.402=-Drosapi.feature.ENABLE_API_KEYS=true
    # Logging (6xx)
    wrapper.java.additional.600=-Drosapi.framework.logger=OFF
    wrapper.java.additional.601=-Djava.util.logging.config.file=../conf/logging.properties
    wrapper.java.additional.602=-Djorg.eclipse.jetty.LEVEL=OFF
    wrapper.java.additional.603=-Dlog4j.configurationFile=../conf/log4j2.xml
    # Suppress warnings in Java 11 runtime.  Only activate with Java 9 or higher.  Remove after WS-2440
    wrapper.java.additional.711.java_version.min=9
    wrapper.java.additional.711=--add-opens=java.base/java.lang=ALL-UNNAMED
    wrapper.java.additional.712.java_version.min=9
    wrapper.java.additional.712=--add-opens=java.base/java.net=ALL-UNNAMED
    # Implicitly flush stdout after each line of output is sent to the console.
    # If piping the console output of the Wrapper process into another application (eg. docker compose)
    # the output won't flush in real time with FALSE
    wrapper.console.flush=TRUE
    #********************************************************************
    # Basis Technology Debugging
    #********************************************************************
    # Uncomment to enable remote debugging
    #wrapper.java.additional.701=-Xdebug -Xnoagent -Djava.compiler=NONE
    #wrapper.java.additional.702=-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005
    # Consult with http://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html
    # and configure remote monitoring and management accordingly.
    #wrapper.java.additional.801=-Dcom.sun.management.jmxremote.port=1099
    # ...
    # if the below isn't set to true, the wrapper will assume process is hung while debugging
    #wrapper.java.detect_debug_jvm=true
    # To debug the requests the server receives, uncomment the following lines
    # and enable the org.apache.cxf.services logger in the log4j2.xml configuration
    #wrapper.java.classpath.3=./cxf-rt-features-logging*.jar
    #wrapper.java.additional.654=-Dorg.apache.cxf.logging.enabled=pretty
    #********************************************************************
    # End
    #********************************************************************
    # Initial Java Heap Size (in MB)
    #wrapper.java.initmemory=3
    wrapper.java.initmemory=8192
    # Maximum Java Heap Size (in MB)
    #wrapper.java.maxmemory=64
    wrapper.java.maxmemory=16384
    # Number of seconds to allow between the time that the Wrapper launches the JVM process
    # The default value is "30 seconds".
    wrapper.startup.timeout=300
    #include ../conf/wrapper-override.conf
    #********************************************************************
    # Wrapper Logging Properties
    #********************************************************************
    # Enables Debug output from the Wrapper.
    # wrapper.debug=TRUE
    # Format of output for the console.  (See docs for formats)
    wrapper.console.format=PM
    # Log Level for console output.  (See docs for log levels)
    wrapper.console.loglevel=INFO
    # Log file to use for wrapper output logging.
    wrapper.logfile=../logs/wrapper.log
    # Format of output for the log file.  (See docs for formats)
    wrapper.logfile.format=LPTM
    # Log Level for log file output.  (See docs for log levels)
    wrapper.logfile.loglevel=INFO
    # Maximum size that the log file will be allowed to grow to before
    #  the log is rolled. Size is specified in bytes.  The default value
    #  of 0, disables log rolling.  May abbreviate with the 'k' (kb) or
    #  'm' (mb) suffix.  For example: 10m = 10 megabytes.
    wrapper.logfile.maxsize=10m
    # Maximum number of rolled log files which will be allowed before old
    #  files are deleted.  The default value of 0 implies no limit.
    wrapper.logfile.maxfiles=5
    # Log Level for sys/event log output.  (See docs for log levels)
    wrapper.syslog.loglevel=NONE
    #********************************************************************
    # Wrapper General Properties
    #********************************************************************
    # Allow for the use of non-contiguous numbered properties
    wrapper.ignore_sequence_gaps=TRUE
    # Do not start if the pid file already exists.
    #wrapper.pidfile.strict=TRUE
    # Title to use when running as a console
    # wrapper.console.title=Rosette Server Edition
    #********************************************************************
    # Wrapper JVM Checks
    #********************************************************************
    # Detect DeadLocked Threads in the JVM. (Requires Standard Edition)
    wrapper.check.deadlock=TRUE
    wrapper.check.deadlock.interval=60
    wrapper.check.deadlock.action=RESTART
    wrapper.check.deadlock.output=FULL
    # Out Of Memory detection.
    #  Ignore -verbose:class output to avoid false positives.
    #wrapper.filter.trigger.1000=[Loaded java.lang.OutOfMemoryError
    #wrapper.filter.action.1000=NONE
    # (Simple match)
    #wrapper.filter.trigger.1001=java.lang.OutOfMemoryError
    # (Only match text in stack traces if -XX:+PrintClassHistogram is being used.)
    #wrapper.filter.trigger.1001=Exception in thread "*" java.lang.OutOfMemoryError
    #wrapper.filter.allow_wildcards.1001=TRUE
    #wrapper.filter.action.1001=RESTART
    #wrapper.filter.message.1001=The JVM has run out of memory.
    #********************************************************************
    # Basis Technology Customizations
    #********************************************************************
    # interferes with license check at top, not really wanted. Karaf should not exit.
    #wrapper.on_exit.default=RESTART
    wrapper.on_exit.3=SHUTDOWN
    # Ping timeout / interval for detecting frozen JVM
    wrapper.ping.timeout=310
    wrapper.ping.interval=10
    wrapper.ping.timeout.action=DEBUG
    #********************************************************************
    # Wrapper Email Notifications. (Requires Professional Edition)
    #********************************************************************
    # Common Event Email settings.
    #wrapper.event.default.email.debug=TRUE
    #wrapper.event.default.email.smtp.host=<SMTP_Host>
    #wrapper.event.default.email.smtp.port=25
    #wrapper.event.default.email.subject=[%WRAPPER_HOSTNAME%:%WRAPPER_NAME%:%WRAPPER_EVENT_NAME%] Event Notification
    #wrapper.event.default.email.sender=<Sender email>
    #wrapper.event.default.email.recipient=<Recipient email>
    # Configure the log attached to event emails.
    #wrapper.event.default.email.maillog=ATTACHMENT
    #wrapper.event.default.email.maillog.lines=50
    #wrapper.event.default.email.maillog.format=LPTM
    #wrapper.event.default.email.maillog.loglevel=INFO
    # Enable specific event emails.
    #wrapper.event.wrapper_start.email=TRUE
    #wrapper.event.jvm_prelaunch.email=TRUE
    #wrapper.event.jvm_start.email=TRUE
    #wrapper.event.jvm_started.email=TRUE
    #wrapper.event.jvm_deadlock.email=TRUE
    #wrapper.event.jvm_stop.email=TRUE
    #wrapper.event.jvm_stopped.email=TRUE
    #wrapper.event.jvm_restart.email=TRUE
    #wrapper.event.jvm_failed_invocation.email=TRUE
    #wrapper.event.jvm_max_failed_invocations.email=TRUE
    #wrapper.event.jvm_kill.email=TRUE
    #wrapper.event.jvm_killed.email=TRUE
    #wrapper.event.jvm_unexpected_exit.email=TRUE
    #wrapper.event.wrapper_stop.email=TRUE
    # Specify custom mail content
    #wrapper.event.jvm_restart.email.body=The JVM was restarted.\n\nPlease check on its status.\n
    #********************************************************************
    # Wrapper Windows Service Properties
    #********************************************************************
    # WARNING - Do not modify any of these properties when an application
    #  using this configuration file has been installed as a service.
    #  Please uninstall the service before modifying this section.  The
    #  service can then be reinstalled.
    # TODO: These won't be right for worker versus front-end, but Windows is not a short-term concern.
    # Name of the service
    wrapper.name=rosapi-launcher
    # Display name of the service
    wrapper.displayname=Rosette Server Edition
    # Description of the service
    wrapper.description=Rosette Server Edition
    # Service dependencies.  Add dependencies as needed starting from 1
    wrapper.ntservice.dependency.1=
    # Mode in which the service is installed.  AUTO_START, DELAY_START or DEMAND_START
    wrapper.ntservice.starttype=AUTO_START
    # Allow the service to interact with the desktop (Windows NT/2000/XP only).
    wrapper.ntservice.interactive=FALSE
    # Allow the current user to perform certain actions without being prompted for
    #  administrator credentials. (Requires Professional Edition)
    #wrapper.ntservice.permissions.1.account=CURRENT_USER
    #wrapper.ntservice.permissions.1.allow=START, STOP, PAUSE_RESUME


